{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pipeline\n# 1 - Augment: \n#         + Augmentation on the fly\n#         + Robust to skin tones\n#         + Hair removal with conditions (resize w.r.t ratio -> chọn lọc những hair thon dài)\n#         + Horizontal Flip\n#         + Random rotate 90\n#         + Affine\n#         + Elastic Transform\n#         + RGB or HSV shift\n#         + Median or Motion Blur\n#         + Coarse Dropout\n# 2 - Preprocessing: \n#         + Clahe on L channel\n#         + Conditional (Median Blur) nếu như std lớn quá thì sẽ blur\n#         + Resize\n#         + Normalize\n# 3 - Loss function: \n#         + Focal Tversky (60%) → Class imbalance + recall focus (catching all lesion pixels).\n#         + Dynamic BCE (20%) → Stabilizes pixel-wise classification, avoids trivial \"all background.\"\n#         + Surface loss (20%) → Fine-tunes edges so the segmentation matches lesion contours.\n# 4 - Model Selection:\n#         + EPolar-ResUnet\n#         + Unet++\n#         + Unet\n#         + PSPNet\n#         + DeepLabv3\n#         + Polar-ResUnet++\n#         + SegFormer\n# 5 - Postprocessing\n#         + CRFs\n#         + mask2rle\n#         + csv submission\n\n\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install git+https://github.com/lucasb-eyer/pydensecrf.git\n!pip install lightning\n!pip install segmentation_models_pytorch\n!pip install albumentations opencv-python","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchmetrics\nfrom torchmetrics import JaccardIndex\nfrom torchmetrics.segmentation import DiceScore\nfrom torchvision.transforms import v2 as T\nimport os\nfrom tqdm import tqdm\nfrom glob import glob\nimport timm\nfrom PIL import Image, ImageEnhance, ImageFilter\nimport random\nfrom sklearn.model_selection import train_test_split\nimport csv\nimport torch.nn.functional as F\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport pytorch_lightning as pl\nimport wandb\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport random\nimport pydensecrf.densecrf as dcrf\nfrom pydensecrf.utils import unary_from_softmax\nimport albumentations as A\nfrom torchmetrics.classification import BinaryJaccardIndex, BinaryF1Score\nfrom pathlib import Path\nfrom albumentations.pytorch import ToTensorV2  \nfrom scipy.ndimage import distance_transform_edt as dted\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T11:24:50.503055Z","iopub.execute_input":"2025-09-08T11:24:50.503582Z","iopub.status.idle":"2025-09-08T11:24:58.537370Z","shell.execute_reply.started":"2025-09-08T11:24:50.503559Z","shell.execute_reply":"2025-09-08T11:24:58.536803Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def read_image(path: Path):\n    \"\"\"Read an image (RGB).\"\"\"\n    img = cv2.imread(str(path), cv2.IMREAD_COLOR)\n    if img is None:\n        raise FileNotFoundError(f\"Image not found: {path}\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\n\ndef read_mask(path: Path, binarize=True):\n    \"\"\"Read a mask (grayscale).\"\"\"\n    mask = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n    if mask is None:\n        raise FileNotFoundError(f\"Mask not found: {path}\")\n    if binarize:\n        mask = (mask > 127).astype(\"uint8\")\n    return mask\ndef plot_image_and_mask(image, mask=None, title=\"Sample\"):\n    \"\"\"Plot image and option mask.\"\"\"\n    plt.figure(figsize=(10, 5))\n\n    # Show image\n    if mask is not None:\n        plt.figure(figsize=(12, 6))\n\n        plt.subplot(1, 2, 1)\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.title(\"Image\")\n\n        plt.subplot(1, 2, 2)\n        plt.imshow(mask, cmap=\"gray\")\n        plt.axis(\"off\")\n        plt.title(\"Mask\")\n    else:\n        plt.figure(figsize=(6, 6))\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.title(\"Image\")\n    plt.suptitle(title)\n    plt.show\ndef show_random_sample(image_paths, mask_paths, title=\"Random Sample\"):\n    \"\"\"Randomly choose one sample (image + mask) and plot it\"\"\"\n    idx = random.randint(0, len(image_paths) - 1)\n    img = read_image(image_paths[idx])\n    mask = read_mask(mask_paths[idx])\n    print(\"mask:\", mask.shape, mask.dtype, mask.min().item(), mask.max().item())\n    print(\"img:\", img.shape, img.dtype, img.min().item(), img.max().item(), img.mean().item())\n    plot_image_and_mask(img, mask, title=f\"{title} - {image_paths[idx].stem}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dir = Path(\"/kaggle/input/warm-up-program-ai-vietnam-skin-segmentation/Test/Test\")\ntrain_dir = Path(\"/kaggle/input/warm-up-program-ai-vietnam-skin-segmentation/Train/Train\")\n\ntrain_image_path_list = sorted(list(train_dir.glob(\"Image/*.jpg\")))\ntrain_mask_path_list = sorted(list(train_dir.glob(\"Mask/*.png\")))\ntest_image_path_list = sorted(list(test_dir.glob(\"Image/*\")))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_image_path_list), len(train_mask_path_list), len(test_image_path_list)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_random_sample(train_image_path_list, train_mask_path_list)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SkinSegmentationDataset(Dataset):\n    def __init__(self, image_paths, mask_paths=None, test=False, transform=None):\n        self.test = test\n        self.transform = transform\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n    def __len__(self):\n        return len(self.image_paths)\n    def __getitem__(self, index: int):\n        image = read_image(self.image_paths[index])\n\n        if self.test:\n            if self.transform:\n                image = self.transform(image=image)[\"image\"]\n            return image\n        else:\n            mask = read_mask(self.mask_paths[index])\n            if self.transform:\n                transformed = self.transform(image=image, mask=mask)\n                image, mask = transformed[\"image\"], transformed[\"mask\"]\n            return image, mask","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def hair_removal(\n    img_path: str,\n    mask_path: str,\n    process_original: bool = False,\n    min_side_for_fast: int = 512,       # when process_original=False, resize so min side == this\n    blackhat_kernel: int = 17,\n    threshold_method: str = \"otsu\", # \"adaptive\" or \"otsu\" or \"fixed\"\n    fixed_thresh: int = 10,\n    min_hair_area_base: int = 100,      # base min area for ~512 input; will scale by image area\n    aspect_ratio_thresh: float = 3.0,   # gentle default\n    compactness_thresh: float = 0.65,   # gentle default (area / (w*h))\n    dilate_iter: int = 1,\n    dilate_kernel: int = 3,\n    inpaint_radius: int = 2,\n    inpaint_method = cv2.INPAINT_NS\n):\n    \"\"\"\n    Detect long/elongated hairs and inpaint. Returns (inpainted_rgb, mask_uint8, confidence).\n    - mask is binary uint8 same size as original image (255 = hair).\n    \"\"\"\n    def read_rgb(path):\n        im = cv2.imread(path, cv2.IMREAD_COLOR)\n        if im is None:\n            raise FileNotFoundError(path)\n        return cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n\n    def read_mask(path):\n        m = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n        if m is None:\n            raise RuntimeError(f\"Cannot read mask: {path}\")\n        if m.ndim == 3:\n            m = cv2.cvtColor(m, cv2.COLOR_BGR2GRAY)\n        return m\n\n    img = read_rgb(img_path)\n    mask = read_mask(mask_path)\n    H, W = img.shape[:2]\n\n    # choose working image (either original or resized while keeping aspect ratio)\n    if process_original:\n        work = img.copy()\n        scale = 1.0\n    else:\n        min_side = min(H, W)\n        if min_side <= min_side_for_fast:\n            work = img.copy()\n            scale = 1.0\n        else:\n            scale = float(1) / int(min_side / float(min_side_for_fast))\n            new_w = int(round(W * scale))\n            new_h = int(round(H * scale))\n            resize_transform = A.Resize(new_h, new_w, interpolation=cv2.INTER_AREA)\n            resize = resize_transform(image=img, mask=mask)\n            work, mask = resize['image'], resize['mask']\n\n    wh, ww = work.shape[:2]\n\n    # grayscale + blackhat\n    gray = cv2.cvtColor(work, cv2.COLOR_RGB2GRAY)\n    k = cv2.getStructuringElement(cv2.MORPH_RECT, (blackhat_kernel, blackhat_kernel))\n    bh = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, k)\n    bh = cv2.normalize(bh, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n\n    # threshold\n    if threshold_method == \"otsu\":\n        thr_val, _ = cv2.threshold(bh, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n        lower = max(thr_val // 3, fixed_thresh)\n        _, th = cv2.threshold(bh, lower, 255, cv2.THRESH_BINARY)\n    elif threshold_method == \"fixed\":\n        _, th = cv2.threshold(bh, fixed_thresh, 255, cv2.THRESH_BINARY)\n    else:  # adaptive\n        # blockSize must be odd and >=3\n        bs = 15 if min(wh, ww) > 100 else 11\n        th = cv2.adaptiveThreshold(bh, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n                                   cv2.THRESH_BINARY, bs, -4)\n\n    # connected components filtering (area + elongation + compactness)\n    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(th, connectivity=8)\n    mask_small = np.zeros_like(th)\n\n    # scale min hair area by image area relative to 512x512\n    scale_area = (wh * ww) / float(512 * 512)\n    min_hair_area = max(8, int(round(min_hair_area_base * scale_area)))\n\n    for i in range(1, num_labels):\n        x, y, w, h, area = stats[i]\n        if area < min_hair_area:\n            continue\n\n        # elongation measure\n        aspect = max(w, h) / (min(w, h) + 1e-8)\n        compactness = area / float(w * h + 1e-8)\n\n        # keep if reasonably elongated OR very large (covering long hair)\n        if (aspect >= aspect_ratio_thresh and compactness <= compactness_thresh) or (area >= 4 * min_hair_area):\n            mask_small[labels == i] = 255\n\n    # small morphological clean + dilate to ensure hair fully covered\n    if np.any(mask_small):\n        mask_small = cv2.morphologyEx(mask_small, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n        mask_small = cv2.dilate(mask_small, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (dilate_kernel, dilate_kernel)), iterations=dilate_iter)\n\n    mask_full = mask_small\n\n    # compute simple detection metrics / confidence\n    total_px = mask_full.size\n    hair_px = int(np.count_nonzero(mask_full))\n    frac = hair_px / float(total_px)\n    mean_bh = float(np.mean(bh)) / 255.0  # relative on working image\n\n    # inpaint on original image (use mask_full). cv2.inpaint expects single-channel mask uint8\n    mask_for_inpaint = (mask_full > 0).astype(np.uint8) * 255\n    inpainted = cv2.inpaint(work, mask_for_inpaint, inpaint_radius, inpaint_method)\n    return inpainted, mask\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD  = (0.229, 0.224, 0.225)\n\n# --- Custom preprocessing ---\ndef conditional_median_blur(img, std_thresh=30, ksize=3):\n    \"\"\"Apply median blur only if image has high noise (std > threshold).\"\"\"\n    if img.std() > std_thresh:\n        return cv2.medianBlur(img, ksize)\n    return img\n\n# Albumentations allows custom transforms using A.Lambda\ndef preprocess_fn(img, **kwargs):\n    # img = remove_hair(img)\n    img = conditional_median_blur(img)\n    return img\n\n# --- Train Augmentations ---\ntrain_transforms = A.Compose([\n    A.Lambda(image=preprocess_fn, p=1.0),  # hair removal + conditional blur\n    A.Resize(512, 512, interpolation=cv2.INTER_LINEAR),\n    A.HorizontalFlip(p=0.5),\n    A.RandomRotate90(p=0.3),\n    A.Affine(\n        scale=(0.9, 1.1),\n        translate_percent=(0.1, 0.1),\n        rotate=(-20, 20),\n        shear=(-10, 10),\n        p=0.7\n    ),\n    A.ElasticTransform(alpha=1, sigma=50, p=0.3),\n    A.OneOf([\n        A.RGBShift(r_shift_limit=20, g_shift_limit=20, b_shift_limit=20, p=0.5),\n        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n    ], p=0.5),\n    A.OneOf([\n        A.MotionBlur(blur_limit=5, p=0.2),\n        A.MedianBlur(blur_limit=3, p=0.2),\n    ], p=0.2),\n    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.3),\n    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n    ToTensorV2()\n])  # ensures mask is transformed consistently\n\n# --- Validation Augmentations ---\nval_transforms = A.Compose([\n    A.Lambda(image=preprocess_fn, p=1.0),\n    A.Resize(512, 512, interpolation=cv2.INTER_LINEAR),\n    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=1.0),  # deterministic\n    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n    ToTensorV2()\n])\n\n# --- Test uses same as validation ---\ntest_transforms = val_transforms","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_img, val_img, train_msk, val_msk = train_test_split(\n    train_image_path_list,\n    train_mask_path_list,\n    test_size=0.1,\n    random_state=42,\n    shuffle=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = SkinSegmentationDataset(\n    image_paths=train_img,\n    mask_paths=train_msk,\n    test=False,\n    transform=train_transforms\n)\n\nval_dataset = SkinSegmentationDataset(\n    image_paths=val_img,\n    mask_paths=val_msk,\n    test=False,\n    transform=val_transforms\n)\n\ntest_dataset = SkinSegmentationDataset(\n    image_paths=test_image_path_list,\n    mask_paths=None,\n    test=True,\n    transform=test_transforms\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img, mask = train_dataset[0]\nvalidation_img, validation_mask = val_dataset[0]\ntesting_img = test_dataset[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_image_and_mask(img.permute(1,2,0), mask)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_image_and_mask(validation_img.permute(1,2,0), validation_mask)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(testing_img.permute(1,2,0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Core Tversky & Focal-Tversky ---\nclass TverskyLoss(nn.Module):\n    def __init__(self, alpha=0.3, beta=0.7, eps=1e-6):\n        super().__init__(); self.a, self.b, self.eps = alpha, beta, eps\n    def forward(self, logits, y):\n        p = torch.sigmoid(logits); y = y.float()\n        tp = (p*y).sum((2,3))\n        fp = (p*(1-y)).sum((2,3))\n        fn = ((1-p)*y).sum((2,3))\n        t = (tp + self.eps) / (tp + self.a*fp + self.b*fn + self.eps)\n        return (1.0 - t).mean()\n\nclass FocalTverskyLoss(nn.Module):\n    def __init__(self, alpha=0.3, beta=0.7, gamma=1.0, eps=1e-6):\n        super().__init__(); self.a, self.b, self.g, self.eps = alpha, beta, gamma, eps\n    def forward(self, logits, y):\n        p = torch.sigmoid(logits); y = y.float()\n        tp = (p*y).sum((2,3))\n        fp = (p*(1-y)).sum((2,3))\n        fn = ((1-p)*y).sum((2,3))\n        t = (tp + self.eps) / (tp + self.a*fp + self.b*fn + self.eps)\n        return (1.0 - t).pow(self.g).mean()\n\n# --- Boundary-aware (Surface) loss: boosts edge accuracy ---\nclass SurfaceLoss(nn.Module):\n    def forward(self, logits, y):\n        p = torch.sigmoid(logits)\n        with torch.no_grad():\n            dts = []\n            for yi in y.squeeze(1).detach().cpu().numpy():\n                pos, neg = dted(yi>0), dted(yi==0)\n                d = pos + neg\n                dts.append(torch.from_numpy(d).unsqueeze(0))\n            dt = torch.stack(dts, 0).to(logits.device).float()\n            dt = dt / (dt.amax(dim=(2,3), keepdim=True) + 1e-6)\n        return ((p - y.float()).abs() * dt).mean()\n\n# --- BCE with dynamic pos_weight per batch (controls FP vs FN) ---\ndef bce_with_logits_dynamic_pw(logits, targets, max_pw=3.0):\n    y = targets.float()\n    p = (y>0.5).float()\n    pos = p.sum()\n    neg = (1-p).sum()\n    # pos_weight = neg/pos (clipped)\n    pw = (neg / (pos + 1e-6)).clamp(min=1.0, max=max_pw)\n    return F.binary_cross_entropy_with_logits(logits, y, pos_weight=pw)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ComboLossGen(nn.Module):\n    \"\"\"\n    0.6 * FocalTversky(alpha=0.3, beta=0.7, gamma=1.0)\n    + 0.2 * BCEWithLogits (dynamic pos_weight)\n    + 0.2 * SurfaceLoss\n    Shapes: logits [B,1,H,W], targets [B,1,H,W] or [B,H,W]\n    \"\"\"\n    def __init__(self, a=0.3, b=0.7, g=1.0, bce_max_pw=3.0, surf_w=0.20):\n        super().__init__()\n        self.ft = FocalTverskyLoss(alpha=a, beta=b, gamma=g)\n        self.surf = SurfaceLoss()\n        self.bce_max_pw = bce_max_pw\n        self.surf_w = surf_w\n\n    def forward(self, logits, targets):\n        if targets.ndim == 3: targets = targets.unsqueeze(1)\n        if logits.ndim == 3: logits = logits.unsqueeze(1)\n        lt = self.ft(logits, targets)\n        lb = bce_with_logits_dynamic_pw(logits, targets, max_pw=self.bce_max_pw)\n        ls = self.surf(logits, targets)\n        return 0.6*lt + 0.2*lb + self.surf_w*ls","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install segmentation_models_pytorch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import segmentation_models_pytorch as smp\n\n# ---------------------------\n# Utils: soft-argmax and polar sampling\n# ---------------------------\ndef soft_argmax_2d(heatmaps: torch.Tensor, eps=1e-6) -> torch.Tensor:\n    # heatmaps: (B,1,H,W) or (B,H,W)\n    if heatmaps.dim() == 4:\n        hmap = heatmaps[:, 0]\n    else:\n        hmap = heatmaps\n    B, H, W = hmap.shape\n    h = hmap.view(B, -1)\n    p = F.softmax(h, dim=1).view(B, H, W)\n    xs = torch.linspace(0, W - 1, W, device=heatmaps.device)\n    ys = torch.linspace(0, H - 1, H, device=heatmaps.device)\n    xs = xs.view(1, 1, W).expand(B, H, W)\n    ys = ys.view(1, H, 1).expand(B, H, W)\n    x = (p * xs).sum(dim=(1, 2))\n    y = (p * ys).sum(dim=(1, 2))\n    return torch.stack([x, y], dim=1)  # (B,2) (x,y)\n\ndef polar_grid(center_xy: torch.Tensor, out_h: int, out_w: int, max_radius: float, H: int, W: int, device) -> torch.Tensor:\n    # center_xy: (B,2) pixel coords (x,y)\n    B = center_xy.shape[0]\n    r = torch.linspace(0.0, max_radius, out_h, device=device)\n    theta = torch.linspace(0.0, 2 * math.pi, out_w, device=device)\n    rr, tt = torch.meshgrid(r, theta, indexing=\"ij\")  # (out_h, out_w)\n    xx = rr * torch.cos(tt)\n    yy = rr * torch.sin(tt)\n    xx = xx.unsqueeze(0).expand(B, -1, -1) + center_xy[:, 0].view(B, 1, 1)\n    yy = yy.unsqueeze(0).expand(B, -1, -1) + center_xy[:, 1].view(B, 1, 1)\n    grid = torch.stack([xx, yy], dim=-1)  # pixel coords\n    # normalize to [-1,1]\n    x = grid[..., 0]; y = grid[..., 1]\n    x_norm = (x / (W - 1)) * 2.0 - 1.0\n    y_norm = (y / (H - 1)) * 2.0 - 1.0\n    return torch.stack([x_norm, y_norm], dim=-1)  # (B, out_h, out_w, 2)\n\ndef polar_transform_batch(img: torch.Tensor, center_xy: torch.Tensor, out_h: int, out_w: int, max_radius: float) -> torch.Tensor:\n    # img: (B,C,H,W)\n    B, C, H, W = img.shape\n    device = img.device\n    grid = polar_grid(center_xy, out_h, out_w, max_radius, H, W, device)\n    polar = F.grid_sample(img, grid, mode=\"bilinear\", padding_mode=\"zeros\", align_corners=True)\n    return polar  # (B,C,out_h,out_w)\n\ndef inverse_polar_to_cartesian(polar_img: torch.Tensor, center_xy: torch.Tensor, H: int, W: int, max_radius: float) -> torch.Tensor:\n    # polar_img: (B, C, out_h, out_w)\n    B, C, out_h, out_w = polar_img.shape\n    device = polar_img.device\n    ys = torch.linspace(0, H - 1, H, device=device)\n    xs = torch.linspace(0, W - 1, W, device=device)\n    yg, xg = torch.meshgrid(ys, xs, indexing='ij')  # (H,W)\n    xg = xg.unsqueeze(0).expand(B, -1, -1)\n    yg = yg.unsqueeze(0).expand(B, -1, -1)\n    cx = center_xy[:, 0].view(B, 1, 1)\n    cy = center_xy[:, 1].view(B, 1, 1)\n    dx = xg - cx\n    dy = yg - cy\n    r = torch.sqrt(dx ** 2 + dy ** 2)\n    theta = (torch.atan2(dy, dx) % (2 * math.pi))\n    # map r,theta to polar pixel indices\n    r_idx = (r / max_radius) * (out_h - 1)\n    t_idx = (theta / (2 * math.pi)) * (out_w - 1)\n    x_norm = (t_idx / (out_w - 1)) * 2.0 - 1.0\n    y_norm = (r_idx / (out_h - 1)) * 2.0 - 1.0\n    grid = torch.stack([x_norm, y_norm], dim=-1)  # (B,H,W,2)\n    cart = F.grid_sample(polar_img, grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n    return cart  # (B,C,H,W)\n\n# ---------------------------\n# Stacked hourglass (small)\n# ---------------------------\nclass ConvBNReLU(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, x): return self.net(x)\n\nclass SimpleHourglass(nn.Module):\n    def __init__(self, in_ch=1, base=32):\n        super().__init__()\n        # 3-level hourglass\n        self.e1 = ConvBNReLU(in_ch, base)\n        self.p1 = nn.MaxPool2d(2)\n        self.e2 = ConvBNReLU(base, base*2)\n        self.p2 = nn.MaxPool2d(2)\n        self.e3 = ConvBNReLU(base*2, base*4)\n        # up\n        self.up2 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)\n        self.u2_conv = ConvBNReLU(base*4, base*2)\n        self.up1 = nn.ConvTranspose2d(base*2, base, 2, stride=2)\n        self.u1_conv = ConvBNReLU(base*2, base)\n        self.out_head = nn.Conv2d(base, 1, 1)\n\n    def forward(self, x):\n        x1 = self.e1(x)\n        x2 = self.e2(self.p1(x1))\n        x3 = self.e3(self.p2(x2))\n        u2 = self.u2_conv(torch.cat([self.up2(x3), x2], dim=1))\n        u1 = self.u1_conv(torch.cat([self.up1(u2), x1], dim=1))\n        heat = self.out_head(u1)\n        return heat, u1  # return heatmap and final feature\n\nclass StackedHourglass(nn.Module):\n    def __init__(self, n_stacks=3, in_ch=1, base=24):\n        super().__init__()\n        self.stacks = nn.ModuleList([SimpleHourglass(in_ch, base) for _ in range(n_stacks)])\n\n    def forward(self, x):\n        heatmaps = []\n        for hg in self.stacks:\n            heat, _ = hg(x)   # each stack sees the same 1-channel input\n            heatmaps.append(heat)\n        return heatmaps\n\n\n# ---------------------------\n# Edge-attending decoder block (UpEdgeAttention)\n# ---------------------------\nclass UpEdgeAttention(nn.Module):\n    def __init__(self, enc_ch, dec_in_ch, out_ch):\n        super().__init__()\n        # upsample encoder skip → out_ch\n        self.enc_up = nn.ConvTranspose2d(enc_ch, out_ch, kernel_size=2, stride=2)\n        # project decoder input → out_ch\n        self.dec_proj = nn.Conv2d(dec_in_ch, out_ch, kernel_size=1)\n        # fusion convs\n        self.conv = nn.Sequential(\n            nn.Conv2d(out_ch * 2, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, enc_feat, dec_feat):\n        enc_up = self.enc_up(enc_feat)\n        dec_proj = self.dec_proj(dec_feat)\n\n        if enc_up.shape[-2:] != dec_proj.shape[-2:]:\n            enc_up = F.interpolate(enc_up, size=dec_proj.shape[-2:], mode='bilinear', align_corners=True)\n\n        edge = enc_up - dec_proj\n        cat = torch.cat([edge, dec_proj], dim=1)\n        return self.conv(cat)\n\n# ---------------------------\n# SMP-based encoder wrapper to expose multi-scale features\n# ---------------------------\nclass SMPEncoderWrapper(nn.Module):\n    def __init__(self, encoder_name=\"resnet34\", in_channels=3, pretrained=True):\n        super().__init__()\n        # create smp Unet but we'll access its encoder blocks\n        model = smp.Unet(encoder_name=encoder_name, encoder_weights=\"imagenet\" if pretrained else None, in_channels=in_channels, classes=1)\n        self.encoder = model.encoder\n        # encoder.out_channels gives channels of each stage (list)\n        self.out_channels = self.encoder.out_channels  # e.g., [64, 64, 128, 256, 512] depending on encoder\n\n    def forward(self, x):\n        # returns list of features from encoder stages (skip connections)\n        features = self.encoder(x)  # list of tensors\n        # features[0] is initial conv output, last is deepest\n        return features\n\n# ---------------------------\n# Polar/Cartesian decoders with edge attention + multi-scale fusion\n# ---------------------------\nclass DecoderWithEdgeFusion(nn.Module):\n    def __init__(self, encoder_channels, decoder_channels):\n        super().__init__()\n        self.num_stages = len(decoder_channels)\n        self.up_blocks = nn.ModuleList()\n\n        dec_in_ch = encoder_channels[-1]  # start with bottleneck channels\n        for i in range(self.num_stages):\n            enc_ch = encoder_channels[-(i+2)]  # skip channel\n            out_ch = decoder_channels[i]\n            self.up_blocks.append(UpEdgeAttention(enc_ch, dec_in_ch, out_ch))\n            dec_in_ch = out_ch  # update for next stage\n\n        self.final_conv = nn.Conv2d(decoder_channels[-1], 1, kernel_size=1)\n\n\n    def forward(self, encoder_features, bottleneck_feat):\n        # encoder_features: list returned by encoder (len L)\n        # bottleneck_feat: deepest feature\n        x = bottleneck_feat\n        out_feats = []\n        for i, up in enumerate(self.up_blocks):\n            # enc feature corresponding to this stage\n            skip = encoder_features[-(i+2)]\n            x = up(skip, x)  # edge-attending fusion\n            out_feats.append(x)\n        # out_feats[-1] is the last decoder level\n        logits = self.final_conv(out_feats[-1])\n        return logits, out_feats  # return logits and intermediate decoded features\n\n# ---------------------------\n# Full EPolar-UNet (paper-close)\n# ---------------------------\nclass EPolarUNetPaper(nn.Module):\n    def __init__(self, in_channels=5, encoder_name=\"resnet34\", n_stacks=3, polar_h=512, polar_w=512):\n        super().__init__()\n        self.in_channels = in_channels\n        self.encoder_name = encoder_name\n        self.polar_h = polar_h\n        self.polar_w = polar_w\n        # pole predictor (stacked hourglass) - expects single-chan input; if rgb convert mean\n        self.hourglass = StackedHourglass(n_stacks=n_stacks, in_ch=1, base=24)\n\n        # SMP encoders for Cartesian and Polar branches (shared architecture, separate weights)\n        self.cart_encoder = SMPEncoderWrapper(encoder_name=encoder_name, in_channels=in_channels, pretrained=True)\n        self.polar_encoder = SMPEncoderWrapper(encoder_name=encoder_name, in_channels=in_channels, pretrained=True)\n\n        enc_ch = self.cart_encoder.out_channels  # list\n        # choose decoder channel sizes (you might tune these to match encoder depth)\n        # We'll use reversed smaller sizes for decoder\n        decoder_channels = [256, 128, 64]  # adjust depending on encoder depth/out_channels\n        # create decoders\n        self.cart_decoder = DecoderWithEdgeFusion(enc_ch, decoder_channels)\n        self.polar_decoder = DecoderWithEdgeFusion(enc_ch, decoder_channels)\n\n        # multi-scale fusion convs: fuse decoder features at each decoder stage\n        # number of decoder stages = len(decoder_channels)\n        self.fusion_convs = nn.ModuleList([nn.Conv2d(c*2, c, 1) for c in decoder_channels])  # careful ordering later\n\n        # final fusion head (after inverse warp of polar logits)\n        self.fusion_head = nn.Sequential(\n            nn.Conv2d(2, 1, 1)\n        )\n\n    def forward(self, x):\n        # x: (B, C, H, W)\n        B, C, H, W = x.shape\n        device = x.device\n\n        # 1) pole heatmaps (stacked hourglass expects single channel)\n        gray = x.mean(dim=1, keepdim=True)  # (B,1,H,W)\n        heatmaps = self.hourglass(gray)  # list of (B,1,H,W)\n        last_hm = heatmaps[-1]\n        coords = soft_argmax_2d(last_hm)  # (B,2) x,y\n\n        # compute max_radius (use diag)\n        max_radius = float(math.sqrt(H*H + W*W))\n\n        # 2) Cartesian branch encoder\n        cart_feats = self.cart_encoder(x)  # list of features length L\n        # deepest bottleneck is last element\n        cart_bottleneck = cart_feats[-1]\n\n        # 3) Polar transform (differentiable)\n        polar_img = polar_transform_batch(x, coords, self.polar_h, self.polar_w, max_radius)  # (B,C,ph,pw)\n\n        # polar branch encoder\n        polar_feats = self.polar_encoder(polar_img)\n        polar_bottleneck = polar_feats[-1]\n\n        # 4) decode both branches with edge-attending decoders\n        cart_logits, cart_decoded_feats = self.cart_decoder(cart_feats, cart_bottleneck)\n        polar_logits, polar_decoded_feats = self.polar_decoder(polar_feats, polar_bottleneck)\n        # cart_decoded_feats & polar_decoded_feats: list length num_stages, each (B, ch, h_i, w_i)\n\n        # 5) Multi-scale fusion: fuse each corresponding decoder feature\n        # We'll upsample/reshape fused features progressively and optionally add to final prediction.\n        fused_feats = []\n        for i in range(len(cart_decoded_feats)):\n            cfeat = cart_decoded_feats[i]\n            pfeat = polar_decoded_feats[i]\n            # if polar feature spatial size != cart feature, resize\n            if pfeat.shape[-2:] != cfeat.shape[-2:]:\n                pfeat = F.interpolate(pfeat, size=cfeat.shape[-2:], mode='bilinear', align_corners=True)\n            fused = torch.cat([cfeat, pfeat], dim=1)  # concat channels\n            # fusion conv (1x1) to reduce\n            fuse_conv = self.fusion_convs[i]\n            fused = fuse_conv(fused)\n            fused_feats.append(fused)\n\n        # 6) Choose a fusion strategy: here we take last fused feature, conv to logits\n        # convert polar_logits (in polar image space) back to cartesian\n        polar_logits_cart = inverse_polar_to_cartesian(polar_logits, coords, H, W, max_radius)  # (B,1,H,W)\n\n        # ensure shapes for concatenation\n        if polar_logits_cart.shape != cart_logits.shape:\n            polar_logits_cart = F.interpolate(polar_logits_cart, size=cart_logits.shape[-2:], mode='bilinear', align_corners=True)\n\n        fused_final = torch.cat([cart_logits, polar_logits_cart], dim=1)  # (B,2,H,W)\n        out = self.fusion_head(fused_final)  # (B,1,H,W)\n        out = F.interpolate(out, size=(H, W), mode=\"bilinear\", align_corners=True)\n        return out, {\n            \"coords\": coords,\n            \"cart_logits\": cart_logits,\n            \"polar_logits_cart\": polar_logits_cart,\n            \"heatmaps\": heatmaps,\n            \"fused_feats\": fused_feats,\n            \"polar_img\": polar_img\n        }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LightningSampleModel(pl.LightningModule):\n    def __init__(self, lr=1e-2, weight_decay=0.0, base_ch=32):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        self.model =  EPolarUNetPaper(\n            in_channels=3,\n            encoder_name=\"resnet34\",\n            n_stacks=2,\n            polar_h=512,\n            polar_w=512\n        ) # <= Remmember to define this to your model\n\n        # reuse your losses\n        self.criterion = ComboLossGen(a=0.3, b=0.7, g=1.0, bce_max_pw=3.0, surf_w=0.20)\n\n        # metrics\n        self.train_iou = BinaryJaccardIndex(threshold=0.5)\n        self.val_iou   = BinaryJaccardIndex(threshold=0.5)\n        self.train_f1  = BinaryF1Score(threshold=0.5)\n        self.val_f1    = BinaryF1Score(threshold=0.5)\n\n    @staticmethod\n    def dice_from_logits(logits, target, eps=1e-7):\n        probs = torch.sigmoid(logits)\n        preds = (probs > 0.5).float().view(probs.size(0), -1)\n        target = target.view(target.size(0), -1).float()\n        inter = (preds * target).sum(dim=1)\n        dice = (2 * inter + eps) / (preds.sum(dim=1) + target.sum(dim=1) + eps)\n        return dice.mean()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def _step(self, batch, stage: str):\n        x, y = batch                    # y: (B,H,W) {0,1}\n        x = x.float()\n        logits = self(x)[0].squeeze(1)     # -> (B,H,W)\n        loss = self.criterion(logits, y)\n        dice = self.dice_from_logits(logits, y)\n        probs = torch.sigmoid(logits)\n\n        if stage == \"train\":\n            self.train_iou.update(probs, y.int()); self.train_f1.update(probs, y.int())\n            self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n            self.log(\"train/dice\", dice, on_step=False, on_epoch=True, prog_bar=True)\n        elif stage == \"val\":\n            self.val_iou.update(probs, y.int()); self.val_f1.update(probs, y.int())\n            self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n            self.log(\"val/dice\", dice, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def training_step(self, batch, batch_idx): return self._step(batch, \"train\")\n    def validation_step(self, batch, batch_idx): self._step(batch, \"val\")\n\n    def on_train_epoch_end(self):\n        self.log(\"train/iou\", self.train_iou.compute(), prog_bar=True)\n        self.log(\"train/f1\",  self.train_f1.compute())\n        self.train_iou.reset(); self.train_f1.reset()\n    def on_validation_epoch_end(self):\n        self.log(\"val/iou\", self.val_iou.compute(), prog_bar=True)\n        self.log(\"val/f1\",  self.val_f1.compute())\n        self.val_iou.reset(); self.val_f1.reset()\n\n    def configure_optimizers(self):\n        # Paper uses Adam; keep your plateau scheduler for simplicity\n        opt = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", patience=3, factor=0.5)\n        return {\"optimizer\": opt, \"lr_scheduler\": {\"scheduler\": sch, \"monitor\": \"val/dice\"}}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SkinDataModule(pl.LightningDataModule):\n    def __init__(self, train_dataset, val_dataset, test_dataset=None, batch_size=5, num_workers=4):\n        super().__init__()\n        self.train_dataset = train_dataset\n        self.val_dataset = val_dataset\n        self.test_dataset = test_dataset\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.num_workers\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers\n        )\n    \n    def test_dataloader(self):\n        if self.test_dataset is None:\n            raise ValueError(\"No test dataset provided!\")\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir checkpoints\n!mkdir checkpoints/epolarunet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"student_id = \"10423057\"  # TODO: replace with your student ID\napi_key = os.environ.get(\"WANDB_API_KEY\", \"83f4544a22543e319c6009abceaac90b634c68a3\")  # configure your wandb key here\n\nif api_key == \"\":\n    raise ValueError(\"Please set your wandb key in the code or in the environment variable WANDB_API_KEY\")\nelse:\n    print(\"WandB API key is set. Proceeding with login...\")\n    \nwandb.login(key=api_key)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Find latest checkpoint (if exists)\ncheckpoint_dir = \"/kaggle/working/checkpoints/epolarunet\"\nckpt_path = None\n\nif os.path.exists(checkpoint_dir):\n    ckpts = [os.path.join(checkpoint_dir, f) for f in os.listdir(checkpoint_dir) if f.endswith(\".ckpt\")]\n    if ckpts:\n        ckpt_path = max(ckpts, key=os.path.getctime)  # latest file by creation time\n\nprint(\"Resuming from checkpoint:\" if ckpt_path else \"No checkpoint found.\", ckpt_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pl.seed_everything(42)\n\ndm = SkinDataModule(train_dataset, val_dataset, test_dataset=test_dataset, batch_size=5, num_workers=os.cpu_count())\nMODEL_NAME = \"EPolar-Unet\" # <= CHANGE YOUR MODEL NAME\nPROJECT_NAME = \"Skin-Lesion Segmentation\" # <= CHANGE YOUR PROJECT NAME\n\nmodel = LightningSampleModel(lr=1e-2, weight_decay=0.0, base_ch=32)\n\n# 1) W&B logger\nwandb_logger = WandbLogger(\n    project=PROJECT_NAME,\n    name=MODEL_NAME,\n    log_model=True  # upload best.ckpt as an artifact\n)\n\n# Optional: track your hyperparams in the run config\nwandb_logger.experiment.config.update({\n    \"lr\": 1e-2,               # paper starts at 1e-2, but you can use 3e-4 if safer\n    \"batch_size\": dm.batch_size,  # e.g. 8 for 512x512 on 15 GB\n    \"model\": MODEL_NAME,      # instead of encoder_name\n    \"base_channels\": 32,      # starting channels in the UNet\n    \"loss\": \"Combo Loss\",\n    \"optimizer\": \"Adam\",      # matches paper\n    \"img_size\": \"512x512\",  # auto from your DataModule\n})\n\n# 2) Callbacks\nckpt_cb = ModelCheckpoint(\n    dirpath='/kaggle/working/checkpoints/epolarunet',\n    monitor=\"val/dice\", mode=\"max\", save_top_k=3, save_last=True,\n    filename=\"epoch{epoch:02d}-valdice{val/dice:.4f}\"\n)\n\nearly_cb = EarlyStopping(monitor=\"val/dice\", mode=\"max\", patience=8)\nlr_cb = LearningRateMonitor(logging_interval=\"epoch\")\n\n# 3) Trainer\ntrainer = pl.Trainer(\n    max_epochs=100,\n    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n    devices=1,\n    precision=32,           # use 32 if AMP causes issues\n    gradient_clip_val=1.0,\n    log_every_n_steps=10,\n    callbacks=[ckpt_cb, early_cb,lr_cb],\n    logger=wandb_logger,\n    fast_dev_run=False\n)\n\n# 6) Continue training\ntrainer.fit(model, datamodule=dm, ckpt_path=ckpt_path)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-08T11:24:37.211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}